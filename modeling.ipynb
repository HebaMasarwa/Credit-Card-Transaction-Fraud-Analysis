{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a96939",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd8cc4",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4cb1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing The Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccdc5db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>...</th>\n",
       "      <th>merch_zipcode</th>\n",
       "      <th>customer_id_str</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>trans_time</th>\n",
       "      <th>hour</th>\n",
       "      <th>time_bin</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>age</th>\n",
       "      <th>age_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>NC</td>\n",
       "      <td>...</td>\n",
       "      <td>28705.0</td>\n",
       "      <td>jennifer_banks_f_psychologist, counselling_mor...</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1900-01-01 00:00:18</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00-02:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>37</td>\n",
       "      <td>30-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>WA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stephanie_gill_f_special educational needs tea...</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1900-01-01 00:00:44</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00-02:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>47</td>\n",
       "      <td>40-49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>ID</td>\n",
       "      <td>...</td>\n",
       "      <td>83236.0</td>\n",
       "      <td>edward_sanchez_m_nature conservation officer_m...</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1900-01-01 00:00:51</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00-02:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>63</td>\n",
       "      <td>60-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>MT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jeremy_white_m_patent attorney_boulder</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1900-01-01 00:01:16</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00-02:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>58</td>\n",
       "      <td>50-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>VA</td>\n",
       "      <td>...</td>\n",
       "      <td>22844.0</td>\n",
       "      <td>tyler_garcia_m_dance movement psychotherapist_...</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1900-01-01 00:03:06</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00-02:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>39</td>\n",
       "      <td>30-39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cc_num                            merchant       category  \\\n",
       "0  2703186189652095          fraud_Rippin, Kub and Mann       misc_net   \n",
       "1      630423337322     fraud_Heller, Gutmann and Zieme    grocery_pos   \n",
       "2    38859492057661                fraud_Lind-Buckridge  entertainment   \n",
       "3  3534093764340240  fraud_Kutch, Hermiston and Farrell  gas_transport   \n",
       "4   375534208663984                 fraud_Keeling-Crist       misc_pos   \n",
       "\n",
       "      amt      first     last gender                        street  \\\n",
       "0    4.97   Jennifer    Banks      F                561 Perry Cove   \n",
       "1  107.23  Stephanie     Gill      F  43039 Riley Greens Suite 393   \n",
       "2  220.11     Edward  Sanchez      M      594 White Dale Suite 530   \n",
       "3   45.00     Jeremy    White      M   9443 Cynthia Court Apt. 038   \n",
       "4   41.96      Tyler   Garcia      M              408 Bradley Rest   \n",
       "\n",
       "             city state  ...  merch_zipcode  \\\n",
       "0  Moravian Falls    NC  ...        28705.0   \n",
       "1          Orient    WA  ...            NaN   \n",
       "2      Malad City    ID  ...        83236.0   \n",
       "3         Boulder    MT  ...            NaN   \n",
       "4        Doe Hill    VA  ...        22844.0   \n",
       "\n",
       "                                     customer_id_str  customer_id  trans_date  \\\n",
       "0  jennifer_banks_f_psychologist, counselling_mor...            0  2019-01-01   \n",
       "1  stephanie_gill_f_special educational needs tea...            1  2019-01-01   \n",
       "2  edward_sanchez_m_nature conservation officer_m...            2  2019-01-01   \n",
       "3             jeremy_white_m_patent attorney_boulder            3  2019-01-01   \n",
       "4  tyler_garcia_m_dance movement psychotherapist_...            4  2019-01-01   \n",
       "\n",
       "            trans_time hour     time_bin  day_of_week  age  age_bin  \n",
       "0  1900-01-01 00:00:18    0  00:00-02:00      Tuesday   37    30-39  \n",
       "1  1900-01-01 00:00:44    0  00:00-02:00      Tuesday   47    40-49  \n",
       "2  1900-01-01 00:00:51    0  00:00-02:00      Tuesday   63    60-69  \n",
       "3  1900-01-01 00:01:16    0  00:00-02:00      Tuesday   58    50-59  \n",
       "4  1900-01-01 00:03:06    0  00:00-02:00      Tuesday   39    30-39  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing The Clean and Encoded Data\n",
    "try:\n",
    "    data = pd.read_csv('credit_card_cleaned.csv')\n",
    "except:\n",
    "    print('Error while loading the file')\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7e240c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender',\n",
       "       'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job',\n",
       "       'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
       "       'merch_zipcode', 'customer_id_str', 'customer_id', 'trans_date',\n",
       "       'trans_time', 'hour', 'time_bin', 'day_of_week', 'age', 'age_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35842f46",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebb9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = data.drop(\"is_fraud\", axis=1)\n",
    "\n",
    "# Target\n",
    "y = data[\"is_fraud\"]\n",
    "\n",
    "# Train-test split (80/20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84202d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "is_fraud\n",
      "0    99.421135\n",
      "1     0.578865\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training set:\n",
      "is_fraud\n",
      "0    99.421116\n",
      "1     0.578884\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set:\n",
      "is_fraud\n",
      "0    99.421212\n",
      "1     0.578788\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check distribution in the original dataset\n",
    "print(\"Original dataset:\")\n",
    "print(y.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Check distribution in training set\n",
    "print(\"\\nTraining set:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Check distribution in test set\n",
    "print(\"\\nTest set:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ff4d1",
   "metadata": {},
   "source": [
    "The dataset was split into training and test sets using an 80/20 ratio with stratification on the target variable (`is_fraud`).  \n",
    "This ensures that the class distribution (fraud ≈ 0.58%, non-fraud ≈ 99.42%) is preserved across both sets, preventing bias during model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7325a3",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1ba96",
   "metadata": {},
   "source": [
    "X_train\n",
    "\n",
    "y_train\n",
    "\n",
    "'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender',\n",
    "       'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job',\n",
    "       'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
    "       'merch_zipcode', 'customer_id_str', 'customer_id', 'trans_date',\n",
    "       'trans_time', 'hour', 'time_bin', 'day_of_week', 'age', 'age_bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47e0a9",
   "metadata": {},
   "source": [
    "**Transaction Amount**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c83690",
   "metadata": {},
   "source": [
    "- Median of Transaction Amount\n",
    "- STD of Transaction Amount\n",
    "- Ratio of Transaction Amount (amount/median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_stats_train = X_train.groupby(\"customer_id\")[\"amt\"].agg(\n",
    "    median_amount=\"median\",\n",
    "    std_amount=\"std\"\n",
    ").fillna(0).reset_index()\n",
    "\n",
    "X_train = X_train.merge(customer_stats_train, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# creating ratio of transaction amount to median\n",
    "X_train[\"amt_ratio_to_median\"] = X_train[\"amt\"] / X_train[\"median_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.merge(customer_stats_train, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "X_test[\"median_amount\"].fillna(X_train[\"median_amount\"].median(), inplace=True)\n",
    "X_test[\"std_amount\"].fillna(X_train[\"std_amount\"].median(), inplace=True)\n",
    "\n",
    "X_test[\"amt_ratio_to_median\"] = X_test[\"amt\"] / X_test[\"median_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[[\"median_amount\", \"std_amount\", \"amt_ratio_to_median\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff5b2c",
   "metadata": {},
   "source": [
    "**Transaction Time**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2dece",
   "metadata": {},
   "source": [
    "- Median of Transaction Time per Bin\n",
    "- STD of Transaction Time per Bin\n",
    "- Fraction of transactions per time interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4f932d",
   "metadata": {},
   "source": [
    "Binning - 2-Hour Time Intervals.\n",
    "\n",
    "The reason for using 2-hour intervals is based on the EDA visualization, which showed that the time intervals with the most fraud cases are 22:00–24:00, 00:00–02:00, and 02:00–04:00. To preserve this information, I chose to keep the 2-hour time bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1210c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(0, 25, 2))  # [0,2,4,...,24] - 2-Hour Time Intervals\n",
    "labels = [f\"{h:02d}:00-{h+2:02d}:00\" for h in bins[:-1]]\n",
    "\n",
    "# Create the 'time_bin' column for X_train\n",
    "X_train[\"time_bin\"] = pd.cut(X_train[\"hour\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Ordinal encoding: map each label to a number\n",
    "time_bin_mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "X_train[\"time_bin_encoded\"] = X_train[\"time_bin\"].map(time_bin_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52039be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"time_bin\"] = pd.cut(X_test[\"hour\"], bins=bins, labels=labels, right=False)\n",
    "X_test[\"time_bin_encoded\"] = X_test[\"time_bin\"].map(time_bin_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abef4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_bin_stats = X_train.groupby([\"customer_id\", \"time_bin_encoded\"])[\"amt\"].agg(\n",
    "    median_amt=\"median\",\n",
    "    std_amt=\"std\"\n",
    ").unstack(fill_value=0)\n",
    "\n",
    "cust_bin_stats.columns = [f\"{stat}_{bin}\" for stat, bin in cust_bin_stats.columns]\n",
    "\n",
    "X_train = X_train.merge(cust_bin_stats, left_on=\"customer_id\", right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.merge(cust_bin_stats, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "\n",
    "X_test.fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute count per customer per time bin\n",
    "cust_bin_counts = X_train.groupby([\"customer_id\", \"time_bin_encoded\"]).size().unstack(fill_value=0)\n",
    "\n",
    "cust_bin_frac = cust_bin_counts.div(cust_bin_counts.sum(axis=1), axis=0)\n",
    "\n",
    "X_train = X_train.merge(cust_bin_frac, left_on=\"customer_id\", right_index=True, how=\"left\", suffixes=('', '_frac'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.merge(cust_bin_frac, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "\n",
    "X_test.fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1237d",
   "metadata": {},
   "source": [
    "I will test the features later and drop any that are unnecessary to reduce the model’s cardinality and keep it simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911d9da",
   "metadata": {},
   "source": [
    "**Day of the Week**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb378b",
   "metadata": {},
   "source": [
    "- Median Transaction Amount per Day of the Week per Customer\n",
    "- STD Transaction Amount per Day of the Week per Customer\n",
    "- Frequency of Spending per Day per Customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e78a6",
   "metadata": {},
   "source": [
    "Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map days to numbers (Monday=0, ..., Sunday=6)\n",
    "day_mapping = {\"Monday\":0, \"Tuesday\":1, \"Wednesday\":2, \"Thursday\":3, \"Friday\":4, \"Saturday\":5, \"Sunday\":6}\n",
    "X_train[\"day_encoded\"] = X_train[\"day_of_week\"].map(day_mapping)\n",
    "X_test[\"day_encoded\"] = X_test[\"day_of_week\"].map(day_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cd90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_day_stats = X_train.groupby([\"customer_id\", \"day_of_week\"])[\"amt\"].agg(\n",
    "    median_amt=\"median\",\n",
    "    std_amt=\"std\"\n",
    ").unstack(fill_value=0)\n",
    "\n",
    "cust_day_stats.columns = [f\"{stat}_{day}\" for stat, day in cust_day_stats.columns]\n",
    "\n",
    "cust_day_counts = X_train.groupby([\"customer_id\", \"day_of_week\"]).size().unstack(fill_value=0)\n",
    "cust_day_frac = cust_day_counts.div(cust_day_counts.sum(axis=1), axis=0)\n",
    "cust_day_frac.columns = [f\"frac_{day}\" for day in cust_day_frac.columns]\n",
    "\n",
    "X_train = X_train.merge(cust_day_stats, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "X_train = X_train.merge(cust_day_frac, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "\n",
    "X_test = X_test.merge(cust_day_stats, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "X_test = X_test.merge(cust_day_frac, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "\n",
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c582e9",
   "metadata": {},
   "source": [
    "**Age**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac6542",
   "metadata": {},
   "source": [
    "- Relative frequency per age bin\n",
    "- Median transaction amount per age bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f39880",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bin_counts_train = X_train.groupby(\"age_bin\")[\"trans_num\"].count()\n",
    "total_transactions_train = len(X_train)\n",
    "age_bin_rel_freq_train = (age_bin_counts_train / total_transactions_train).reset_index(name=\"age_bin_rel_freq\")\n",
    "\n",
    "X_train = X_train.merge(age_bin_rel_freq_train, on=\"age_bin\", how=\"left\")\n",
    "\n",
    "X_test = X_test.merge(age_bin_rel_freq_train, on=\"age_bin\", how=\"left\")\n",
    "\n",
    "age_bin_median_train = X_train.groupby(\"age_bin\")[\"amt\"].median().reset_index(name=\"age_bin_median_amt\")\n",
    "\n",
    "X_train = X_train.merge(age_bin_median_train, on=\"age_bin\", how=\"left\")\n",
    "\n",
    "X_test = X_test.merge(age_bin_median_train, on=\"age_bin\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef6b3d",
   "metadata": {},
   "source": [
    "**Categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ffa96f",
   "metadata": {},
   "source": [
    "- Transaction counts per category per customer\n",
    "- Fraction of transactions per category\n",
    "- Median / std transaction amount per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f27db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct categories in the data set: 14\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of distinct categories in the data set: {len(X_train[\"category\"].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ade593",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_cat_counts = X_train.groupby([\"customer_id\", \"category\"]).size().unstack(fill_value=0)\n",
    "X_train = X_train.merge(cust_cat_counts, on=\"customer_id\", how=\"left\")\n",
    "X_test = X_test.merge(cust_cat_counts, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "cust_cat_frac = cust_cat_counts.div(cust_cat_counts.sum(axis=1), axis=0).reset_index()\n",
    "X_train = X_train.merge(cust_cat_frac, on=\"customer_id\", how=\"left\", suffixes=('', '_frac'))\n",
    "X_test = X_test.merge(cust_cat_frac, on=\"customer_id\", how=\"left\", suffixes=('', '_frac'))\n",
    "\n",
    "cat_stats = X_train.groupby([\"customer_id\", \"category\"])[\"amt\"].agg(\n",
    "    median_amt=\"median\",\n",
    "    std_amt=\"std\"\n",
    ").unstack(fill_value=0)\n",
    "X_train = X_train.merge(cat_stats, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "X_test = X_test.merge(cat_stats, left_on=\"customer_id\", right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba9d83",
   "metadata": {},
   "source": [
    "**City Population**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9dc9d3",
   "metadata": {},
   "source": [
    "- Average transaction amount per customer in small vs large cities\n",
    "- Fraction of transactions in high-population (large) cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example threshold for small vs large city (adjust as needed)\n",
    "threshold = data[\"city_pop\"].median()  # median population\n",
    "\n",
    "# Create a city size column\n",
    "X_train[\"city_size\"] = X_train[\"city_pop\"].apply(lambda x: \"small\" if x <= threshold else \"large\")\n",
    "\n",
    "# --- Average transaction amount per customer in small vs large cities ---\n",
    "cust_city_avg = X_train.groupby([\"customer_id\", \"city_size\"])[\"amt\"].mean().unstack(fill_value=0)\n",
    "cust_city_avg.columns = [f\"avg_amt_{size}_city\" for size in cust_city_avg.columns]\n",
    "\n",
    "# Merge into X_train\n",
    "X_train = X_train.merge(cust_city_avg, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "\n",
    "# --- Fraction of transactions in high-population (large) cities ---\n",
    "cust_city_counts = X_train.groupby([\"customer_id\", \"city_size\"]).size().unstack(fill_value=0)\n",
    "cust_city_frac = cust_city_counts.div(cust_city_counts.sum(axis=1), axis=0)\n",
    "cust_city_frac.columns = [f\"frac_txn_{size}_city\" for size in cust_city_frac.columns]\n",
    "\n",
    "# Merge fraction features into X_train\n",
    "X_train = X_train.merge(cust_city_frac, left_on=\"customer_id\", right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51450610",
   "metadata": {},
   "source": [
    "**States**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e42d9d",
   "metadata": {},
   "source": [
    "- Median amount per state\n",
    "- Std amount per state\n",
    "- Fraction of transactions per state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5487a01",
   "metadata": {},
   "source": [
    "One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "787412ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct states in the data set: 51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of distinct states in the data set: {len(X_train[\"state\"].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c91513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode states for X_train\n",
    "state_dummies = pd.get_dummies(X_train[\"state\"], prefix=\"state\")\n",
    "X_train = pd.concat([X_train, state_dummies], axis=1)\n",
    "\n",
    "# --- Customer-level aggregated features per state ---\n",
    "# Median spending per state\n",
    "cust_state_median = X_train.groupby([\"customer_id\", \"state\"])[\"amt\"].median().unstack(fill_value=0)\n",
    "cust_state_median.columns = [f\"median_amt_{s}\" for s in cust_state_median.columns]\n",
    "\n",
    "# Std spending per state\n",
    "cust_state_std = X_train.groupby([\"customer_id\", \"state\"])[\"amt\"].std().fillna(0).unstack(fill_value=0)\n",
    "cust_state_std.columns = [f\"std_amt_{s}\" for s in cust_state_std.columns]\n",
    "\n",
    "# Fraction of transactions per state\n",
    "cust_state_counts = X_train.groupby([\"customer_id\", \"state\"]).size().unstack(fill_value=0)\n",
    "cust_state_frac = cust_state_counts.div(cust_state_counts.sum(axis=1), axis=0)\n",
    "cust_state_frac.columns = [f\"frac_txn_{s}\" for s in cust_state_frac.columns]\n",
    "\n",
    "# Merge all features into X_train\n",
    "X_train = X_train.merge(cust_state_median, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "X_train = X_train.merge(cust_state_std, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "X_train = X_train.merge(cust_state_frac, left_on=\"customer_id\", right_index=True, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c05a82",
   "metadata": {},
   "source": [
    "**Merchants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de58f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct merchants data set: 693\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of distinct merchants in the data set: {len(X_train[\"merchant\"].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3cde0c",
   "metadata": {},
   "source": [
    "**Jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c167f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct jobs in the data set: 494\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of distinct jobs in the data set: {len(X_train[\"job\"].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0aab6",
   "metadata": {},
   "source": [
    "**Cities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "183d1c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct cities in the data set: 894\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of distinct cities in the data set: {len(X_train[\"city\"].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd8535",
   "metadata": {},
   "source": [
    "Merchants, Jobs, Cities all have a high number of unqiue values thus in order to keep the model simple and focus on higher signal features I'll drop these three as features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
